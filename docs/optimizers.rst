.. _optimizers:

==========
Optimizers
==========

.. contents:: :local:

Adadelta
--------

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


Adagrad
-------

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


Adam
----

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


Conjugate Gradients
-------------------

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


.. _optimizers_lbfgs:

BFGS
----

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


Momentum
--------

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


Nesterov Momentum
-----------------

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


Newton's Method
---------------

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


RMSProp
-------

Be the first to `contribute! <https://github.com/bfortuner/ml-cheatsheet>`__


SGD
---

Stochastic Gradient Descent.

.. literalinclude:: ../code/optimizers.py
    :language: python
    :pyobject: SGD


.. rubric:: References

.. [1] http://sebastianruder.com/optimizing-gradient-descent/
.. [2] http://www.deeplearningbook.org/contents/optimization.html
.. [3] https://arxiv.org/pdf/1502.03167.pdf
