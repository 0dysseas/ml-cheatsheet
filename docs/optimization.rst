.. _optimizers:

============
Optimization
============

.. contents:: :local:


General
========

Batch Normalization
-------------------

Be the first to contribute!

First-order Methods
-------------------

Be the first to contribute!

Second-order Methods
--------------------

Be the first to contribute!


Optimizers
==========


Adadelta
--------

Be the first to contribute!


Adagrad
-------

Be the first to contribute!


Adam
----

Be the first to contribute!


Conjugate Gradients
-------------------

Be the first to contribute!


.. _optimizers_lbfgs:

BFGS
----

Be the first to contribute!


Momentum
--------

Be the first to contribute!


Nesterov Momentum
-----------------

Be the first to contribute!


Newton's Method
---------------

Be the first to contribute!


RMSProp
-------

Be the first to contribute!


Stochastic Gradient Descent
---------------------------

.. literalinclude:: ../code/optimizers.py
    :language: python
    :pyobject: SGD



Weight Initialization
=====================

`Good reference <https://github.com/alykhantejani/nninit/blob/master/nninit.py>`_

Kaiming (He)
------------

Normal and uniform varieties..

Normal
------

Be the first to contribute!

Orthogonal
----------

Be the first to contribute!

Sparse
------

Be the first to contribute!

Uniform
-------

Be the first to contribute!

Xavier
------

Normal and uniform varieties..



.. rubric:: References

.. [1] http://sebastianruder.com/optimizing-gradient-descent/
.. [2] http://www.deeplearningbook.org/contents/optimization.html
.. [3] https://arxiv.org/pdf/1502.03167.pdf
